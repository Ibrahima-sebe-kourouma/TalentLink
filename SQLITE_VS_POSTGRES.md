# ğŸ—„ï¸ SQLite vs PostgreSQL pour TalentLink

## ğŸ“‹ Situation actuelle

Tu utilises :
- **SQLite** pour : auth, profile, offers, appointment, report
- **MongoDB** pour : messaging (conversations en temps rÃ©el)

---

## âœ… OPTION 1 : GARDER SQLITE + MONGODB (RecommandÃ© pour dÃ©buter)

### Avantages
- âœ… **Aucune migration** de donnÃ©es nÃ©cessaire
- âœ… Configuration **ultra simple**
- âœ… Fichiers de base de donnÃ©es **faciles Ã  sauvegarder** (simple copie)
- âœ… **IdÃ©al pour dÃ©marrer** et tester en production
- âœ… Pas de serveur DB externe Ã  gÃ©rer

### InconvÃ©nients
- âš ï¸ **Limite de concurrence** : SQLite verrouille le fichier lors des Ã©critures
- âš ï¸ **Performance** : moins bon avec >100 utilisateurs simultanÃ©s
- âš ï¸ **Backup** : manuel (mais simple avec Docker volumes)
- âš ï¸ **ScalabilitÃ©** : difficile si grosse croissance

### Quand utiliser SQLite ?
- âœ… Lancement du projet (MVP)
- âœ… <50-100 utilisateurs simultanÃ©s
- âœ… Application principalement en lecture
- âœ… Tu veux dÃ©ployer vite sans complexitÃ©

---

## ğŸš€ OPTION 2 : MIGRER VERS POSTGRESQL

### Avantages
- âœ… **Concurrence excellente** : plusieurs utilisateurs simultanÃ©s sans problÃ¨me
- âœ… **Performance** : optimisÃ© pour production
- âœ… **Transactions ACID** complÃ¨tes
- âœ… **Backup automatique** : pg_dump, rÃ©plication
- âœ… **ScalabilitÃ©** : peut gÃ©rer des millions de lignes

### InconvÃ©nients
- âš ï¸ **Migration nÃ©cessaire** : transfert des donnÃ©es SQLite â†’ PostgreSQL
- âš ï¸ Plus complexe Ã  configurer
- âš ï¸ NÃ©cessite un conteneur supplÃ©mentaire (ou service cloud)

### Quand migrer vers PostgreSQL ?
- ğŸ¯ >100 utilisateurs simultanÃ©s
- ğŸ¯ Grosse quantitÃ© de donnÃ©es (>2GB)
- ğŸ¯ Beaucoup d'Ã©critures concurrentes
- ğŸ¯ Besoin de rÃ©plication/haute disponibilitÃ©

---

## ğŸ”§ CONFIGURATION ACTUELLE (SQLite + MongoDB)

### Docker Compose adaptÃ©

```yaml
services:
  # MongoDB pour messaging
  mongodb:
    image: mongo:7
    volumes:
      - mongodb_data:/data/db
    ports:
      - "127.0.0.1:27017:27017"

  # Chaque service utilise SQLite
  service_auth:
    volumes:
      - ./service_auth/auth.db:/app/auth.db  # Fichier SQLite persistant

  service_offers:
    volumes:
      - ./service_offers/offers.db:/app/offers.db
      - ./uploads:/app/uploads  # CVs et lettres de motivation
```

### Ce qui se passe avec les bases de donnÃ©es :

1. **Les fichiers `.db` restent SUR TON VPS** (via Docker volumes)
2. Tes donnÃ©es actuelles peuvent Ãªtre copiÃ©es sur le serveur
3. MongoDB tourne dans un conteneur sÃ©parÃ© pour le messaging

---

## ğŸ“¦ MIGRATION DES DONNÃ‰ES (si tu choisis PostgreSQL plus tard)

### Ã‰tape 1 : Exporter SQLite

```bash
# Pour chaque base de donnÃ©es
sqlite3 auth.db .dump > auth_dump.sql
sqlite3 profile.db .dump > profile_dump.sql
sqlite3 offers.db .dump > offers_dump.sql
```

### Ã‰tape 2 : Convertir pour PostgreSQL

Outil recommandÃ© : **pgloader**

```bash
# Installer pgloader
apt install pgloader

# Convertir
pgloader auth.db postgresql://user:pass@localhost/talentlinkdb
```

### Ã‰tape 3 : Adapter le code

Aucun changement de code nÃ©cessaire avec SQLAlchemy ! Juste changer la `DATABASE_URL` :

```python
# Avant (SQLite)
DATABASE_URL = "sqlite:///./auth.db"

# AprÃ¨s (PostgreSQL)
DATABASE_URL = "postgresql://talentlink:password@postgres:5432/talentlinkdb"
```

---

## ğŸ¯ RECOMMANDATION POUR TOI

### Phase 1 : Lancement (maintenant)
**â†’ Garde SQLite + MongoDB**
- DÃ©ploie avec la config actuelle
- Teste en conditions rÃ©elles
- Collecte des mÃ©triques d'usage

### Phase 2 : Si succÃ¨s (dans 3-6 mois)
**â†’ Migre vers PostgreSQL** quand :
- Tu dÃ©passes 50 utilisateurs actifs simultanÃ©s
- Tu vois des ralentissements
- Tu veux automatiser les backups
- Tu lÃ¨ves des fonds ou passes en prod sÃ©rieuse

---

## ğŸ”„ BACKUP AVEC SQLITE

### Backup automatique (cron sur le VPS)

CrÃ©e `/root/backup_sqlite.sh` :

```bash
#!/bin/bash
BACKUP_DIR="/home/backups/talenlink"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p $BACKUP_DIR

# Copier toutes les bases SQLite
cp /home/talenlink/backend/service_auth/auth.db $BACKUP_DIR/auth_$DATE.db
cp /home/talenlink/backend/service_profile/profile.db $BACKUP_DIR/profile_$DATE.db
cp /home/talenlink/backend/service_offers/offers.db $BACKUP_DIR/offers_$DATE.db
cp /home/talenlink/backend/service_appointment/appointment.db $BACKUP_DIR/appointment_$DATE.db
cp /home/talenlink/backend/service_report/report.db $BACKUP_DIR/report_$DATE.db

# Compresser
tar -czf $BACKUP_DIR/talenlink_backup_$DATE.tar.gz $BACKUP_DIR/*.db
rm $BACKUP_DIR/*.db

# Garder seulement les 7 derniers jours
find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete

echo "Backup completed: $DATE"
```

Ajoute au cron :

```bash
chmod +x /root/backup_sqlite.sh
crontab -e

# Ajoute cette ligne : backup quotidien Ã  2h du matin
0 2 * * * /root/backup_sqlite.sh
```

### Backup MongoDB

```bash
# Dump MongoDB
docker exec mongodb mongodump --out=/dump --username=talentlink --password=changeme123

# Copier depuis le conteneur
docker cp mongodb:/dump /home/backups/mongodb_$(date +%Y%m%d)
```

---

## ğŸ“Š MONITORING AVEC SQLITE

### VÃ©rifier la taille des bases

```bash
# Sur le VPS
du -sh /home/talenlink/backend/service_*/*.db

# Exemple de sortie :
# 2.4M  service_auth/auth.db
# 1.8M  service_profile/profile.db
# 15M   service_offers/offers.db
```

### Performance check

Si une DB dÃ©passe **500MB**, considÃ¨re PostgreSQL.

---

## âš¡ OPTIMISATION SQLITE POUR PRODUCTION

Dans chaque `database/database.py`, ajoute ces paramÃ¨tres :

```python
from sqlalchemy import create_engine, event

engine = create_engine(
    "sqlite:///./auth.db",
    connect_args={
        "check_same_thread": False,
        # Optimisations SQLite
        "timeout": 30,  # Attendre 30s si verrouillÃ©
    },
    pool_pre_ping=True,
    pool_recycle=3600,
)

# Activer WAL mode (Write-Ahead Logging) pour meilleures perfs
@event.listens_for(engine, "connect")
def set_sqlite_pragma(dbapi_conn, connection_record):
    cursor = dbapi_conn.cursor()
    cursor.execute("PRAGMA journal_mode=WAL")
    cursor.execute("PRAGMA synchronous=NORMAL")
    cursor.execute("PRAGMA cache_size=-64000")  # 64MB cache
    cursor.close()
```

---

## ğŸ“ RÃ‰SUMÃ‰

| CritÃ¨re | Ta situation |
|---------|-------------|
| **Base actuelle** | SQLite + MongoDB âœ… |
| **Migration obligatoire ?** | âŒ NON |
| **Action immÃ©diate** | DÃ©ployer avec SQLite + MongoDB |
| **Quand migrer ?** | Quand >50 users simultanÃ©s ou DB >500MB |
| **ComplexitÃ©** | Faible (juste changer DATABASE_URL) |

**â†’ Commence avec SQLite, migre vers PostgreSQL si nÃ©cessaire plus tard.**

C'est comme acheter une voiture : commence avec une Civic (SQLite), upgrade vers une Mercedes (PostgreSQL) quand tu rÃ©ussis ! ğŸš—â†’ğŸï¸
