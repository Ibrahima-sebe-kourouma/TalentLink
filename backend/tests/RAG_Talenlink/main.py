import os
from pathlib import Path
import base64
from pdf2image import convert_from_path
import chromadb
from openai import OpenAI
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Initialisation du client OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class TalenlinkRAG:
    def __init__(self, data_dir="./data", persist_dir="./chroma_db"):
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        
        print("üîÑ Initialisation du RAG TalentLink avec Vision.....")
        
        # Initialiser ChromaDB
        self.client_chroma = chromadb.PersistentClient(path=str(self.persist_dir))
        self.collection = self.client_chroma.get_or_create_collection(
            name="talenlink_docs",
            metadata={"description": "Documentation TalentLink avec images"}
        )
        
        print("‚úÖ RAG initialis√©\n")
    
    def encode_image(self, image_path):
        """Encoder une image en base64 pour l'API OpenAI"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def extract_text_from_pdf_with_vision(self, pdf_path):
        """Extraire le texte ET analyser les images avec GPT-4 Vision"""
        print(f"   üìÑ Conversion du PDF en images...")
        
        # Convertir le PDF en images (une image par page)
        try:
            images = convert_from_path(pdf_path, dpi=200)
        except Exception as e:
            print(f"   ‚ùå Erreur conversion PDF: {e}")
            return []
        
        print(f"   ‚Üí {len(images)} pages √† analyser")
        
        all_page_texts = []
        
        for i, image in enumerate(images):
            print(f"   üîç Analyse de la page {i+1}/{len(images)} avec GPT-4 Vision...")
            
            # Sauvegarder temporairement l'image
            temp_image_path = f"temp_page_{i}.png"
            image.save(temp_image_path, 'PNG')
            
            # Encoder l'image en base64
            base64_image = self.encode_image(temp_image_path)
            
            try:
                # Utiliser GPT-4 Vision pour extraire TOUT le contenu de la page
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": (
                                        "Analyse cette page de documentation technique et extrais TOUT le contenu:\n"
                                        "- Le texte √©crit\n"
                                        "- Les diagrammes (d√©cris-les en d√©tail: cas d'utilisation, s√©quence, classes, etc.)\n"
                                        "- Les tableaux (reproduis leur structure)\n"
                                        "- Les sch√©mas et graphiques (explique ce qu'ils repr√©sentent)\n"
                                        "- Les relations entre les √©l√©ments\n\n"
                                        "Sois tr√®s d√©taill√© et exhaustif. C'est pour un syst√®me RAG."
                                    )
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=4096
                )
                
                page_content = response.choices[0].message.content
                all_page_texts.append({
                    'page': i + 1,
                    'content': page_content
                })
                
                print(f"   ‚úÖ Page {i+1} analys√©e ({len(page_content)} caract√®res extraits)")
                
            except Exception as e:
                print(f"   ‚ùå Erreur analyse page {i+1}: {e}")
            
            # Supprimer l'image temporaire
            try:
                os.remove(temp_image_path)
            except:
                pass
        
        return all_page_texts
    
    def chunk_text(self, text, chunk_size=1000, overlap=100):
        """D√©couper le texte en chunks avec overlap"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk)
        
        return chunks
    
    def create_embedding(self, text):
        """Cr√©er un embedding avec OpenAI"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    
    def load_documents(self):
        """Charger tous les PDFs et cr√©er la base vectorielle"""
        print("üìö Chargement des documents avec analyse Vision...\n")
        
        # V√©rifier si la collection est d√©j√† remplie
        if self.collection.count() > 0:
            print(f"‚úÖ Base vectorielle d√©j√† charg√©e avec {self.collection.count()} chunks\n")
            return
        
        pdf_files = list(self.data_dir.glob("*.pdf"))
        if not pdf_files:
            print("‚ùå Aucun fichier PDF trouv√© dans ./data")
            return
        
        all_chunks = []
        all_metadatas = []
        all_ids = []
        all_embeddings = []
        
        for pdf_file in pdf_files:
            print(f"üìÑ Traitement: {pdf_file.name}")
            
            # Extraire le contenu avec Vision (inclut texte + analyse d'images)
            pages_content = self.extract_text_from_pdf_with_vision(pdf_file)
            
            if not pages_content:
                print(f"   ‚ö†Ô∏è Aucun contenu extrait")
                continue
            
            # Traiter chaque page
            for page_data in pages_content:
                page_num = page_data['page']
                content = page_data['content']
                
                # D√©couper en chunks si le contenu est trop long
                chunks = self.chunk_text(content, chunk_size=1000, overlap=100)
                
                print(f"   ‚Üí Page {page_num}: {len(chunks)} chunk(s) cr√©√©(s)")
                
                # Cr√©er les embeddings
                for i, chunk in enumerate(chunks):
                    embedding = self.create_embedding(chunk)
                    
                    all_chunks.append(chunk)
                    all_embeddings.append(embedding)
                    all_metadatas.append({
                        "source": pdf_file.name,
                        "page": page_num,
                        "chunk_id": i
                    })
                    all_ids.append(f"{pdf_file.stem}_page{page_num}_chunk{i}")
            
            print(f"   ‚úÖ {pdf_file.name} trait√©\n")
        
        # Ajouter √† ChromaDB
        if all_chunks:
            print(f"üíæ Stockage de {len(all_chunks)} chunks dans ChromaDB...")
            self.collection.add(
                embeddings=all_embeddings,
                documents=all_chunks,
                metadatas=all_metadatas,
                ids=all_ids
            )
            print(f"‚úÖ Base vectorielle cr√©√©e avec {len(all_chunks)} chunks!\n")
        else:
            print("‚ùå Aucun chunk √† stocker")
    
    def search_context(self, query, n_results=5):
        """Rechercher les chunks les plus pertinents"""
        # Cr√©er l'embedding de la question
        query_embedding = self.create_embedding(query)
        
        # Rechercher dans ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        # Formater le contexte
        context = ""
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
            context += f"\n--- Extrait {i+1} (Source: {metadata['source']}, Page {metadata['page']}) ---\n"
            context += doc + "\n"
        
        return context
    
    def ask_with_context(self, question):
        """Poser une question avec le contexte RAG"""
        print("üîç Recherche dans la documentation...\n")
        
        # R√©cup√©rer le contexte pertinent
        context = self.search_context(question)
        
        # Cr√©er le prompt avec contexte
        system_context = (
            "Tu es un assistant expert sur le projet TalentLink. "
            "Utilise UNIQUEMENT les informations fournies dans le contexte ci-dessous pour r√©pondre. "
            "Si l'information n'est pas dans le contexte, dis-le clairement. "
            "R√©ponds en fran√ßais de mani√®re claire et concise.\n\n"
            f"CONTEXTE:\n{context}"
        )
        
        print("ü§ñ G√©n√©ration de la r√©ponse avec GPT-4o-mini...\n")
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_context},
                {"role": "user", "content": question}
            ]
        )
        
        return response.choices[0].message.content


def main():
    print("=" * 60)
    print("üöÄ RAG TalentLink - Assistant Documentation (OpenAI)")
    print("=" * 60 + "\n")
    
    # Initialiser le RAG
    rag = TalenlinkRAG()
    
    # Charger les documents
    rag.load_documents()
    
    # Boucle interactive
    print("üí¨ Pose tes questions sur TalentLink (tape 'quit' pour quitter)\n")
    
    while True:
        question = input("\n‚ùì Ta question: ")
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("\nüëã Au revoir!")
            break
        
        if not question.strip():
            continue
        
        print()
        try:
            response = rag.ask_with_context(question)
            print("üìù R√©ponse:")
            print("-" * 60)
            print(response)
            print("-" * 60)
        except Exception as e:
            print(f"‚ùå Erreur: {e}")


if __name__ == "__main__":
    main()
